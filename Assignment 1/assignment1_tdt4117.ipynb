{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00a8eb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1: Boolean Model, TF-IDF, and Data Retrieval vs. Information Retrieval Conceptual Questions\n",
    "\n",
    "**Student names**: _Your_names_here_ <br>\n",
    "**Group number**: _Your_group_here_ <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 14.09.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement a Boolean retrieval model\n",
    "- Compute TF-IDF vectors for documents\n",
    "- Run retrieval on queries\n",
    "- Answer conceptual questions \n",
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249058",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773d293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8f900",
   "metadata": {},
   "source": [
    "## 1.1 – Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81516f89",
   "metadata": {},
   "source": [
    "### 1.1.1 Tokenize documents\n",
    "\n",
    "Implement tokenization using the given list of stopwords. Create a list of normalized terms per document (e.g., lowercase, remove punctuation/digits; drop stopwords). Store the token lists to use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d78a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement tokenization using the given list of stopwords, create list of terms per document\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "# Your code here\n",
    "\n",
    "import string\n",
    "\n",
    "def tokenization(docs):\n",
    "    \"\"\"\n",
    "    Tokenize documents from Cranfield corpus dictionary.\n",
    "    \n",
    "    Args:\n",
    "        docs (dict): Dictionary from parse_cranfield() with document data\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tokenized terms per document (stopwords removed)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for doc in docs.values():\n",
    "        # Combine title and abstract\n",
    "        text = doc[\"title\"] + \" \" + doc[\"abstract\"]\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation and split into words\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        \n",
    "        # Split into tokens\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in STOPWORDS and token.strip()]\n",
    "        \n",
    "        result.append(filtered_tokens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_documents = tokenization(docs)  # <-- This creates tokenized_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f40d",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Create a set (or list) of unique terms from all tokenized documents. Report the number of unique terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9cc192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 7361\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a set or list of unique terms\n",
    "\n",
    "# Report: \n",
    "# - Number of unique terms\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def build_vocabulary(tokenized_documents):\n",
    "    \"\"\"\n",
    "    Build vocabulary from tokenized documents.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_documents (list): List of tokenized documents (list of lists)\n",
    "        \n",
    "    Returns:\n",
    "        set: Set of unique terms from all documents\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for document_tokens in tokenized_documents:\n",
    "        vocabulary.update(document_tokens)\n",
    "        \n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(tokenized_documents)  # Receives the returned set\n",
    "print(f\"Number of unique terms: {len(vocabulary)}\")  # Uses the returned set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02912",
   "metadata": {},
   "source": [
    "### Build inverted index\n",
    "\n",
    "For each term, store the list (or set) of document IDs where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms in index: 7361\n",
      "Term 'wing' appears in documents: {0, 519, 12, 13, 29, 30, 544, 546, 1061, 1063, 41, 560, 1073, 1074, 51, 59, 1088, 1089, 1090, 1091, 68, 1093, 1094, 75, 77, 598, 599, 600, 1110, 1114, 91, 94, 1127, 631, 632, 1143, 635, 637, 642, 1161, 1162, 1163, 1167, 1168, 145, 146, 1169, 670, 672, 673, 674, 675, 676, 1185, 1187, 679, 680, 681, 682, 1196, 1201, 691, 692, 693, 694, 695, 1206, 697, 698, 1207, 188, 190, 702, 704, 1217, 194, 708, 198, 199, 710, 201, 711, 203, 204, 713, 1228, 1232, 1238, 1242, 221, 1245, 223, 224, 225, 228, 229, 234, 746, 747, 748, 751, 1265, 754, 756, 245, 246, 1270, 249, 251, 1275, 1276, 255, 767, 1279, 1288, 1289, 778, 780, 782, 278, 790, 792, 793, 794, 283, 795, 796, 286, 287, 288, 800, 802, 807, 808, 810, 1327, 1330, 1332, 310, 1335, 1336, 1337, 1339, 1340, 1341, 1342, 332, 1361, 859, 1379, 876, 882, 378, 894, 900, 901, 394, 915, 916, 917, 918, 919, 920, 922, 923, 926, 415, 928, 419, 431, 432, 433, 441, 452, 969, 970, 463, 990, 485, 496, 511}\n",
      "'experimental' -> documents: {0, 10, 11, 16, 18, 24, 28, 29, 34, 40, 41, 46, 51, 52, 57, 68, 69, 73, 77, 83, 98, 100, 102, 111, 114, 120, 122, 136, 139, 141, 153, 155, 167, 169, 170, 172, 175, 178, 182, 183, 185, 186, 187, 188, 190, 194, 196, 201, 202, 205, 206, 211, 215, 219, 221, 224, 226, 229, 233, 244, 250, 255, 256, 261, 270, 272, 276, 281, 282, 285, 293, 294, 303, 306, 328, 329, 333, 337, 338, 343, 344, 345, 346, 353, 359, 368, 369, 371, 376, 396, 408, 410, 412, 417, 419, 420, 422, 426, 434, 438, 440, 441, 452, 454, 461, 463, 466, 483, 493, 495, 496, 497, 500, 502, 503, 504, 510, 517, 519, 521, 535, 539, 543, 548, 551, 552, 557, 562, 566, 568, 571, 575, 587, 594, 599, 605, 609, 631, 633, 634, 635, 643, 644, 648, 661, 662, 665, 669, 674, 677, 678, 684, 687, 688, 693, 703, 711, 712, 716, 719, 727, 728, 738, 739, 742, 752, 759, 765, 766, 771, 780, 789, 800, 801, 805, 815, 819, 822, 824, 826, 828, 829, 835, 843, 844, 845, 846, 855, 856, 857, 862, 865, 866, 868, 877, 880, 886, 890, 906, 910, 911, 922, 923, 926, 927, 931, 934, 945, 949, 950, 953, 954, 958, 960, 963, 964, 973, 983, 985, 995, 996, 998, 1005, 1007, 1015, 1018, 1027, 1038, 1039, 1044, 1045, 1048, 1050, 1061, 1065, 1068, 1069, 1073, 1074, 1075, 1077, 1079, 1080, 1081, 1082, 1091, 1096, 1097, 1111, 1117, 1121, 1124, 1126, 1144, 1145, 1150, 1152, 1154, 1155, 1157, 1158, 1159, 1160, 1166, 1170, 1176, 1184, 1185, 1186, 1191, 1194, 1195, 1197, 1198, 1203, 1204, 1208, 1211, 1212, 1213, 1215, 1217, 1219, 1221, 1224, 1226, 1227, 1229, 1230, 1233, 1236, 1260, 1261, 1262, 1263, 1267, 1268, 1276, 1289, 1297, 1301, 1309, 1313, 1336, 1337, 1338, 1340, 1351, 1362, 1363, 1368, 1371, 1373, 1383, 1389, 1391, 1395, 1396}\n",
      "'investigation' -> documents: {0, 512, 7, 8, 521, 526, 1038, 18, 28, 29, 548, 1061, 1062, 551, 1064, 1065, 43, 44, 1071, 49, 1073, 1074, 565, 566, 55, 568, 1077, 570, 1082, 1090, 1091, 1093, 1094, 72, 73, 1096, 1097, 1099, 77, 78, 79, 81, 83, 88, 89, 1113, 1115, 1118, 1129, 1143, 633, 634, 635, 125, 637, 127, 128, 642, 1155, 134, 1158, 1160, 1161, 650, 1162, 1163, 1164, 654, 1165, 661, 664, 672, 164, 169, 172, 173, 175, 688, 178, 691, 692, 693, 694, 183, 695, 1204, 186, 187, 188, 699, 1212, 1219, 196, 197, 708, 709, 710, 201, 711, 712, 204, 1224, 206, 1226, 1229, 1230, 211, 212, 213, 1238, 215, 221, 1246, 224, 738, 227, 746, 242, 244, 245, 756, 758, 1270, 1271, 250, 251, 1273, 765, 259, 771, 264, 779, 781, 279, 795, 796, 1308, 800, 1312, 803, 1316, 293, 1318, 807, 808, 1319, 810, 811, 1322, 814, 815, 311, 1336, 825, 1337, 1340, 1342, 835, 1348, 1351, 840, 1352, 1353, 843, 338, 1363, 341, 854, 1366, 857, 858, 1372, 1376, 1380, 1382, 874, 1386, 364, 1394, 371, 373, 886, 891, 389, 904, 906, 926, 931, 932, 422, 426, 432, 433, 945, 435, 951, 952, 441, 958, 959, 969, 970, 971, 461, 462, 473, 985, 991, 992, 993, 995, 996, 1000, 496, 504, 1018}\n",
      "'aerodynamics' -> documents: {0, 901, 10, 791, 283, 32, 288, 295, 296, 684, 688, 1330, 1205, 1346, 452, 215, 224, 1379, 359, 236, 752, 243, 1270, 633}\n",
      "'wing' -> documents: {0, 519, 12, 13, 29, 30, 544, 546, 1061, 1063, 41, 560, 1073, 1074, 51, 59, 1088, 1089, 1090, 1091, 68, 1093, 1094, 75, 77, 598, 599, 600, 1110, 1114, 91, 94, 1127, 631, 632, 1143, 635, 637, 642, 1161, 1162, 1163, 1167, 1168, 145, 146, 1169, 670, 672, 673, 674, 675, 676, 1185, 1187, 679, 680, 681, 682, 1196, 1201, 691, 692, 693, 694, 695, 1206, 697, 698, 1207, 188, 190, 702, 704, 1217, 194, 708, 198, 199, 710, 201, 711, 203, 204, 713, 1228, 1232, 1238, 1242, 221, 1245, 223, 224, 225, 228, 229, 234, 746, 747, 748, 751, 1265, 754, 756, 245, 246, 1270, 249, 251, 1275, 1276, 255, 767, 1279, 1288, 1289, 778, 780, 782, 278, 790, 792, 793, 794, 283, 795, 796, 286, 287, 288, 800, 802, 807, 808, 810, 1327, 1330, 1332, 310, 1335, 1336, 1337, 1339, 1340, 1341, 1342, 332, 1361, 859, 1379, 876, 882, 378, 894, 900, 901, 394, 915, 916, 917, 918, 919, 920, 922, 923, 926, 415, 928, 419, 431, 432, 433, 441, 452, 969, 970, 463, 990, 485, 496, 511}\n",
      "'slipstream' -> documents: {0, 1088, 1089, 483, 452, 1090, 1091, 1063, 1093, 1163, 1164, 1165, 1143, 408}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: For each term, store list of document IDs where the term appears\n",
    "# Your code here\n",
    "\n",
    "def build_inverted_index(tokenized_documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index from tokenized documents.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_documents (list): List of tokenized documents (list of lists)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary where keys are terms and values are sets of document IDs\n",
    "              that contain the term\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "    \n",
    "    for doc_id, document_tokens in enumerate(tokenized_documents):\n",
    "        for term in document_tokens:\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = set()\n",
    "            inverted_index[term].add(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = build_inverted_index(tokenized_documents)\n",
    "print(f\"Number of unique terms in index: {len(inverted_index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e81bf",
   "metadata": {},
   "source": [
    "### Retrieve documents for a Boolean query (AND/OR)\n",
    "\n",
    "Create a function to retrieve documents for a Boolean query (AND/OR) with query terms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9c9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function for retrieving documents for a Boolean query (AND/OR) with query terms\n",
    "\n",
    "def boolean_retrieve(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve documents for a Boolean query (AND/OR) with query terms.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Boolean query string (e.g., \"wing AND flutter\", \"heat OR transfer\")\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of document IDs that match the Boolean query\n",
    "    \"\"\"\n",
    "    # Convert query to lowercase and strip whitespace\n",
    "    query = query.lower().strip()\n",
    "    \n",
    "    # Handle empty query\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    # Split query into tokens\n",
    "    tokens = query.split()\n",
    "    \n",
    "    # Filter out stopwords but keep AND/OR operators\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in ['and', 'or'] or (token not in STOPWORDS and token.strip()):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    if not filtered_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Handle OR operations first (lower precedence)\n",
    "    # Split by OR and process each OR group with AND operations\n",
    "    or_groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for token in filtered_tokens:\n",
    "        if token == 'or':\n",
    "            if current_group:\n",
    "                or_groups.append(current_group)\n",
    "                current_group = []\n",
    "        else:\n",
    "            current_group.append(token)\n",
    "    \n",
    "    # Add the last group\n",
    "    if current_group:\n",
    "        or_groups.append(current_group)\n",
    "    \n",
    "    # Process each OR group (handle AND operations within each group)\n",
    "    final_results = set()\n",
    "    \n",
    "    for group in or_groups:\n",
    "        # Filter out 'and' tokens and keep only search terms\n",
    "        terms = [token for token in group if token != 'and']\n",
    "        \n",
    "        if not terms:\n",
    "            continue\n",
    "            \n",
    "        # For AND operation: start with first term and intersect with others\n",
    "        group_result = inverted_index.get(terms[0], set()).copy()\n",
    "        \n",
    "        for term in terms[1:]:\n",
    "            term_docs = inverted_index.get(term, set())\n",
    "            group_result = group_result.intersection(term_docs)\n",
    "        \n",
    "        # Union this group's results with final results (OR operation between groups)\n",
    "        final_results = final_results.union(group_result)\n",
    "    \n",
    "    # Convert to sorted list before returning\n",
    "    return sorted(list(final_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bf47585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "boolean_queries = [\n",
    "  \"gas AND pressure\",\n",
    "  \"structural AND aeroelastic AND flight AND high AND speed OR aircraft\",\n",
    "  \"heat AND conduction AND composite AND slabs\",\n",
    "  \"boundary AND layer AND control\",\n",
    "  \"compressible AND flow AND nozzle\",\n",
    "  \"combustion AND chamber AND injection\",\n",
    "  \"laminar AND turbulent AND transition\",\n",
    "  \"fatigue AND crack AND growth\",\n",
    "  \"wing AND tip AND vortices\",\n",
    "  \"propulsion AND efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [26, 48, 84, 100, 109]\n",
      "Q2 => [11, 13, 28, 46, 50]\n",
      "Q3 => [4, 398]\n",
      "Q4 => [0, 60, 243, 264, 341]\n",
      "Q5 => [117, 130]\n",
      "Q6 => []\n",
      "Q7 => [6, 8, 79, 88, 95]\n",
      "Q8 => []\n",
      "Q9 => [674]\n",
      "Q10 => [967]\n"
     ]
    }
   ],
   "source": [
    "# Run Boolean queries in batch, using the function you created\n",
    "def run_batch_boolean(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = boolean_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "boolean_results = run_batch_boolean(boolean_queries)\n",
    "for qid, res in boolean_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b591b81",
   "metadata": {},
   "source": [
    "## Part 1.2 – TF-IDF Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed448f",
   "metadata": {},
   "source": [
    "\n",
    "$tf_{i,j} = \\text{Raw Frequency}$\n",
    "\n",
    "$idf_t = \\log\\left(\\frac{N}{df_t}\\right)$\n",
    "\n",
    "### Build document–term matrix (TF and IDF weights)\n",
    "\n",
    "Compute tf and idf using the formulas above and store the weights in a document–term matrix (rows = documents, columns = terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the weights for the documents and the terms using tf and idf weighting. Put these values into a document–term matrix (rows = documents, columns = terms).\n",
    "\n",
    "# Your code here\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tf_idf_matrix():\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF weights using previously created variables:\n",
    "    - tokenized_documents (from tokenization cell)\n",
    "    - vocabulary (from build_vocabulary cell)\n",
    "    - inverted_index (from inverted index cell)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tf_idf_matrix, terms_list)\n",
    "    \"\"\"\n",
    "    # Convert vocabulary to sorted list for consistent column ordering\n",
    "    terms_list = sorted(list(vocabulary))\n",
    "    num_docs = len(tokenized_documents)\n",
    "    num_terms = len(terms_list)\n",
    "    \n",
    "    # Create term to index mapping\n",
    "    term_to_idx = {term: idx for idx, term in enumerate(terms_list)}\n",
    "    \n",
    "    # Initialize TF-IDF matrix\n",
    "    tf_idf_matrix = np.zeros((num_docs, num_terms))\n",
    "    \n",
    "    # Calculate TF-IDF for each document\n",
    "    for doc_idx, doc_tokens in enumerate(tokenized_documents):\n",
    "        # Calculate term frequency (tf) for this document\n",
    "        term_counts = Counter(doc_tokens)\n",
    "        \n",
    "        for term in term_counts:\n",
    "            if term in term_to_idx:\n",
    "                term_idx = term_to_idx[term]\n",
    "                \n",
    "                # TF: Raw frequency\n",
    "                tf = term_counts[term]\n",
    "                \n",
    "                # IDF: log(N / df_t) - use inverted_index to get df_t\n",
    "                df_t = len(inverted_index[term])  # Number of docs containing this term\n",
    "                idf = math.log(num_docs / df_t)\n",
    "                \n",
    "                # TF-IDF weight\n",
    "                tf_idf = tf * idf\n",
    "                \n",
    "                tf_idf_matrix[doc_idx, term_idx] = tf_idf\n",
    "    \n",
    "    return tf_idf_matrix, terms_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007cbf7",
   "metadata": {},
   "source": [
    "### Build TF–IDF document vectors\n",
    "\n",
    "From the matrix, build a TF–IDF vector for each document (consider normalization if needed for cosine similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Build TF–IDF document vectors from the document–term matrix\n",
    "# Your code here\n",
    "\n",
    "def build_document_vectors(tf_idf_matrix, normalize=True):\n",
    "    \"\"\"\n",
    "    Build TF-IDF document vectors from the document-term matrix.\n",
    "    \n",
    "    Args:\n",
    "        tf_idf_matrix (np.array): TF-IDF matrix (rows = docs, cols = terms)\n",
    "        normalize (bool): Whether to normalize vectors for cosine similarity\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Document vectors (normalized if specified)\n",
    "    \"\"\"\n",
    "    document_vectors = tf_idf_matrix.copy()\n",
    "    \n",
    "    if normalize:\n",
    "        # L2 normalization for each document vector (row)\n",
    "        # This prepares vectors for cosine similarity calculations\n",
    "        norms = np.linalg.norm(document_vectors, axis=1, keepdims=True)\n",
    "        # Avoid division by zero for empty documents\n",
    "        norms = np.where(norms == 0, 1, norms)\n",
    "        document_vectors = document_vectors / norms\n",
    "    \n",
    "    return document_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df320c",
   "metadata": {},
   "source": [
    "### Implement cosine similarity\n",
    "\n",
    "Implement a function to compute cosine similarity scores between a (tokenized) query and all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Create a function for calculating the similarity score of all the documents by their relevance to query terms\n",
    "\n",
    "def tfidf_retrieve(query: str):\n",
    "    # Your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4968750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "tfidf_queries = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18861681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TF-IDF queries in batch (print top-5 results for each), using the function you created\n",
    "def run_batch_tfidf(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = tfidf_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "tfidf_results = run_batch_tfidf(tfidf_queries)\n",
    "\n",
    "for qid, res in tfidf_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0989101",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1.3 – Conceptual Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the difference between data retrieval and information retrieval?**\n",
    "*Your answer here*\n",
    "\n",
    "**For the following scenarios, which approach would be suitable data retrieval or information retrieval? Explain your reasoning.** <br>\n",
    "1.a A clerk in pharmacy uses the following query: Medicine_name = Ibuprofen_400mg\n",
    "*Your answer here*\n",
    "\n",
    "1.b A clerk in pharmacy uses the following query: An anti-biotic medicine \n",
    "*Your answer here*\n",
    "\n",
    "1.c Searching for the schedule of a flight using the following query: Flight_ID = ZEFV2\n",
    "*Your answer here*\n",
    "\n",
    "1.d Searching an E-commerce website using the following query to find an specific shoe: Brooks Ghost 15\n",
    "*Your answer here*\n",
    "\n",
    "1.e Searching the same E-commerce website using the following query: Nice running shoes\n",
    "*Your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
