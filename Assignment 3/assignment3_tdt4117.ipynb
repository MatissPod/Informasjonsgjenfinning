{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39093417",
   "metadata": {},
   "source": [
    "# Assignment 3: Local Association Matrix \n",
    "\n",
    "**Student names**: Matiss Podins <br>\n",
    "**Group number**: 39 <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please read and follow these rules. Submissions that do not fulfill them may be returned.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. Submit in **.ipynb** format only.\n",
    "3. The assignment must be typed. Handwritten answers are not accepted.\n",
    "\n",
    "**Due date**: 12.10.2025 23:59\n",
    "\n",
    "### What you will do \n",
    "- Build a **local association matrix** from Cranfield collection.\n",
    "- Compute the **normalized association matrix**.\n",
    "- Use the normalized matrix to **identify neighborhood terms** for expansion for given queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0b1f",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10841b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7263b",
   "metadata": {},
   "source": [
    "## 3.1  Local association matrix\n",
    "\n",
    "For the given Cranfield document collection in cran.all.1400 construct a local association matrix to identify association clusters. Use the docs variable with the parsed file. Omit stopwords in the STOPWORDS list given below from the vocabulary. \n",
    "\n",
    "\n",
    "The correlation factors $c_{u,v}$ between any pair of terms $w_u$ and $w_v$ are defined as  \n",
    "$c_{u,v} = \\sum_{d_j \\in D} f_{u,j} \\cdot f_{v,j}$  \n",
    "\n",
    "$f_{u,j}$ is the raw term frequency of $w_u$ in document $d_j$.\n",
    "\n",
    "### Weighting variants: **scalar** and **metric**\n",
    "\n",
    "Add two alternative weighting schemes for the matrix (only the formula for assigning the matrix cell value changes):\n",
    "\n",
    "- **Metric weighting** :\n",
    "Let $w_u(n,j)$ and $w_v(m,j)$ denote the $n$-th and $m$-th occurrences of terms $w_u$ and $w_v$ in document $d_j$.  \n",
    "Define a distance function $r(w_u(n,j), w_v(m,j))$ (e.g., $r(i,k) = 1 + |i - k|$).  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\sum_{d_j \\in D} \\sum_n \\sum_m \\frac{1}{r(w_u(n,j), w_v(m,j))}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Scalar weighting** :\n",
    "Let $\\vec{s}_u = \\langle c_{u,x_1}, c_{u,x_2}, \\dots, c_{u,x_n} \\rangle$ be the neighborhood vector of term $w_u$, and similarly for $w_v$.  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\frac{\\vec{s}_u \\cdot \\vec{s}_v}{|\\vec{s}_u| \\cdot |\\vec{s}_v|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9433bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct a local association matrix for the cranfield collection. Use both weighting variants.\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "# Your code here\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "\n",
    "def tokenization(docs):\n",
    "    result = []\n",
    "    \n",
    "    for doc in docs.values():\n",
    "        # Combine title and abstract\n",
    "        text = doc[\"title\"] + \" \" + doc[\"abstract\"]\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation and split into words\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        \n",
    "        # Split into tokens\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in STOPWORDS and token.strip()]\n",
    "        \n",
    "        result.append(filtered_tokens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_documents = tokenization(docs)\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_documents: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"Build a vocabulary mapping terms to indices.\"\"\"\n",
    "    vocab = set()\n",
    "    for tokens in tokenized_documents:\n",
    "        vocab.update(tokens)\n",
    "    return {term: idx for idx, term in enumerate(sorted(vocab))}\n",
    "\n",
    "def compute_term_frequencies(tokenized_documents: List[List[str]], \n",
    "                            vocab: Dict[str, int]) -> List[Dict[int, int]]:\n",
    "    \"\"\"Compute raw term frequencies for each document.\"\"\"\n",
    "    doc_term_freqs = []\n",
    "    for tokens in tokenized_documents:\n",
    "        freq = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                freq[vocab[token]] += 1\n",
    "        doc_term_freqs.append(dict(freq))\n",
    "    return doc_term_freqs\n",
    "\n",
    "def compute_term_positions(tokenized_documents: List[List[str]], \n",
    "                          vocab: Dict[str, int]) -> List[Dict[int, List[int]]]:\n",
    "    \"\"\"Compute positions of each term in each document.\"\"\"\n",
    "    doc_term_positions = []\n",
    "    for tokens in tokenized_documents:\n",
    "        positions = defaultdict(list)\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token in vocab:\n",
    "                positions[vocab[token]].append(pos)\n",
    "        doc_term_positions.append(dict(positions))\n",
    "    return doc_term_positions\n",
    "\n",
    "def basic_correlation_matrix(doc_term_freqs: List[Dict[int, int]], \n",
    "                             vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"Compute basic correlation matrix using raw term frequencies.\"\"\"\n",
    "    C = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for freq_dict in doc_term_freqs:\n",
    "        terms = list(freq_dict.keys())\n",
    "        for i, u in enumerate(terms):\n",
    "            for v in terms[i:]:  # Only compute upper triangle\n",
    "                weight = freq_dict[u] * freq_dict[v]\n",
    "                C[u, v] += weight\n",
    "                if u != v:\n",
    "                    C[v, u] += weight\n",
    "    \n",
    "    return C\n",
    "\n",
    "def metric_correlation_matrix(doc_term_positions: List[Dict[int, List[int]]], \n",
    "                              vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"Compute metric-weighted correlation matrix (optimized).\"\"\"\n",
    "    C = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for pos_dict in doc_term_positions:\n",
    "        terms = list(pos_dict.keys())\n",
    "        for i, u in enumerate(terms):\n",
    "            pos_u = np.array(pos_dict[u])\n",
    "            for v in terms[i:]:  # Only compute upper triangle\n",
    "                pos_v = np.array(pos_dict[v])\n",
    "                distances = 1 + np.abs(pos_u[:, None] - pos_v[None, :])\n",
    "                weight = np.sum(1.0 / distances)\n",
    "                C[u, v] += weight\n",
    "                if u != v:\n",
    "                    C[v, u] += weight  # Mirror to lower triangle\n",
    "    \n",
    "    return C\n",
    "\n",
    "def scalar_correlation_matrix(C_basic: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute scalar-weighted correlation matrix using cosine similarity.\"\"\"\n",
    "    norms = np.linalg.norm(C_basic, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # Avoid division by zero\n",
    "    normalized = C_basic / norms\n",
    "    C_scalar = normalized @ normalized.T\n",
    "    return C_scalar\n",
    "\n",
    "def build_association_matrices(tokenized_documents: List[List[str]]) -> Tuple:\n",
    "    \"\"\"Build all three types of association matrices.\"\"\"\n",
    "    vocab = build_vocabulary(tokenized_documents)\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    doc_term_freqs = compute_term_frequencies(tokenized_documents, vocab)\n",
    "    C_basic = basic_correlation_matrix(doc_term_freqs, vocab_size)\n",
    "    \n",
    "    doc_term_positions = compute_term_positions(tokenized_documents, vocab)\n",
    "    C_metric = metric_correlation_matrix(doc_term_positions, vocab_size)\n",
    "    \n",
    "    C_scalar = scalar_correlation_matrix(C_basic)\n",
    "    \n",
    "    return vocab, C_basic, C_metric, C_scalar\n",
    "\n",
    "\n",
    "def find_top_associations(C: np.ndarray, vocab: Dict[str, int], \n",
    "                         term: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Find top k associated terms for a given term.\"\"\"\n",
    "    if term not in vocab:\n",
    "        return []\n",
    "    \n",
    "    idx = vocab[term]\n",
    "    correlations = C[idx, :]\n",
    "    \n",
    "    idx_to_term = {idx: term for term, idx in vocab.items()}\n",
    "    top_indices = np.argsort(correlations)[::-1]\n",
    "    \n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        if i != idx and len(results) < k:\n",
    "            results.append((idx_to_term[i], correlations[i]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build all three association matrices\n",
    "vocab, C_basic, C_metric, C_scalar = build_association_matrices(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55f9bc",
   "metadata": {},
   "source": [
    "## 3.2 Normalized association matrix\n",
    "\n",
    "Compute the normalized association matrix from the unnormalized matrix computed above. \n",
    "\n",
    "To normalize the matrix use the following formula: <br>\n",
    "$c'_{u,v} = \\frac{c_{u,v}}{c_{u,u} + c_{v,v} - c_{u,v}}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1f5964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (7361, 7361)\n",
      "\n",
      "Unnormalized matrix (top-left 5x5):\n",
      "[[2.269e+03 1.800e+01 6.400e+01 2.000e+00 2.000e+00]\n",
      " [1.800e+01 8.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [6.400e+01 0.000e+00 3.580e+02 0.000e+00 0.000e+00]\n",
      " [2.000e+00 0.000e+00 0.000e+00 2.000e+00 0.000e+00]\n",
      " [2.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00]]\n",
      "\n",
      "Normalized matrix (top-left 5x5):\n",
      "[[1.00000000e+00 7.96812749e-03 2.49707374e-02 8.81445571e-04\n",
      "  8.81834215e-04]\n",
      " [7.96812749e-03 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.49707374e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [8.81445571e-04 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [8.81834215e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00]]\n",
      "\n",
      "Diagonal samples (first 5): [1. 1. 1. 1. 1.]\n",
      "------------------------------------------------------------\n",
      "Matrix shape: (7361, 7361)\n",
      "\n",
      "Unnormalized matrix (top-left 5x5):\n",
      "[[626.81486208   1.91941155   3.8784562    1.           1.        ]\n",
      " [  1.91941155   8.           0.           0.           0.        ]\n",
      " [  3.8784562    0.         131.56709906   0.           0.        ]\n",
      " [  1.           0.           0.           2.           0.        ]\n",
      " [  1.           0.           0.           0.           1.        ]]\n",
      "\n",
      "Normalized matrix (top-left 5x5):\n",
      "[[1.         0.00303275 0.00514041 0.00159283 0.00159537]\n",
      " [0.00303275 1.         0.         0.         0.        ]\n",
      " [0.00514041 0.         1.         0.         0.        ]\n",
      " [0.00159283 0.         0.         1.         0.        ]\n",
      " [0.00159537 0.         0.         0.         1.        ]]\n",
      "\n",
      "Diagonal samples (first 5): [1. 1. 1. 1. 1.]\n",
      "------------------------------------------------------------\n",
      "Matrix shape: (7361, 7361)\n",
      "\n",
      "Unnormalized matrix (top-left 5x5):\n",
      "[[1.         0.70764835 0.6250777  0.30027814 0.1761558 ]\n",
      " [0.70764835 1.         0.54356953 0.22699571 0.10501406]\n",
      " [0.6250777  0.54356953 1.         0.25878659 0.08840758]\n",
      " [0.30027814 0.22699571 0.25878659 1.         0.0570479 ]\n",
      " [0.1761558  0.10501406 0.08840758 0.0570479  1.        ]]\n",
      "\n",
      "Normalized matrix (top-left 5x5):\n",
      "[[1.         0.54756641 0.45462766 0.1766631  0.0965849 ]\n",
      " [0.54756641 1.         0.37322038 0.12802886 0.0554168 ]\n",
      " [0.45462766 0.37322038 1.         0.14862428 0.04624813]\n",
      " [0.1766631  0.12802886 0.14862428 1.         0.02936145]\n",
      " [0.0965849  0.0554168  0.04624813 0.02936145 1.        ]]\n",
      "\n",
      "Diagonal samples (first 5): [1. 1. 1. 1. 1.]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#TODO: Compute the normalized association matrix \n",
    "\n",
    "# Your code here\n",
    "def normalize_association_matrix(C: np.ndarray, show_preview: bool = True) -> np.ndarray:\n",
    "    \"\"\"Normalize association matrix using Dice coefficient.\n",
    "    c'_{u,v} = c_{u,v} / (c_{u,u} + c_{v,v} - c_{u,v})\n",
    "    \"\"\"\n",
    "    # Get diagonal elements\n",
    "    diag = np.diag(C)\n",
    "    \n",
    "    # Broadcasting: diag[:, None] creates column, diag[None, :] creates row\n",
    "    denominator = diag[:, None] + diag[None, :] - C\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    C_normalized = np.zeros_like(C)\n",
    "    mask = denominator > 0\n",
    "    C_normalized[mask] = C[mask] / denominator[mask]\n",
    "    \n",
    "    if show_preview:\n",
    "        print(f\"Matrix shape: {C.shape}\")\n",
    "        print(f\"\\nUnnormalized matrix (top-left 5x5):\")\n",
    "        print(C[:5, :5])\n",
    "        print(f\"\\nNormalized matrix (top-left 5x5):\")\n",
    "        print(C_normalized[:5, :5])\n",
    "        print(f\"\\nDiagonal samples (first 5): {np.diag(C_normalized)[:5]}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return C_normalized\n",
    "\n",
    "# Normalize the association matrices\n",
    "C_basic_normalized = normalize_association_matrix(C_basic)\n",
    "C_metric_normalized = normalize_association_matrix(C_metric)\n",
    "C_scalar_normalized = normalize_association_matrix(C_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6e8c0",
   "metadata": {},
   "source": [
    "## 3.3 Neighborhood terms\n",
    "\n",
    "With the help of the normalized local association matrix, identify the neighborhood terms that should be used for expansion for the following queries (queries_assignment3):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e52e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "queries_assignment3 = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a58c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'gas pressure':\n",
      "  gas → equilibrium, air, injection, ideal, real\n",
      "  pressure → number, mach, jet, flow, results\n",
      "  → Expansion terms: air, equilibrium, flow, ideal, injection, jet, mach, number, real, results\n",
      "\n",
      "'structural aeroelastic flight high speed aircraft':\n",
      "  structural → loads, fatigue, aircraft, structure, random\n",
      "  aeroelastic → thermo, piston, responses, stations, entirely\n",
      "  flight → altitude, high, 000, test, speed\n",
      "  high → speed, may, numbers, speeds, effects\n",
      "  speed → high, low, speeds, characteristics, effect\n",
      "  aircraft → vtol, structural, ground, structure, slipstream\n",
      "  → Expansion terms: 000, aircraft, altitude, characteristics, effect, effects, entirely, fatigue, ground, high, loads, low, may, numbers, piston, random, responses, slipstream, speed, speeds, stations, structural, structure, test, thermo, vtol\n",
      "\n",
      "'heat conduction composite slabs':\n",
      "  heat → transfer, temperature, laminar, layer, boundary\n",
      "  conduction → trail, controlled, radiation, solid, variational\n",
      "  composite → slab, medium, periodic, refractory, slabs\n",
      "  slabs → refractory, shielded, melting, composite, input\n",
      "  → Expansion terms: boundary, composite, controlled, input, laminar, layer, medium, melting, periodic, radiation, refractory, shielded, slab, slabs, solid, temperature, trail, transfer, variational\n",
      "\n",
      "'boundary layer control':\n",
      "  boundary → layer, laminar, flow, number, wall\n",
      "  layer → boundary, laminar, flow, number, transfer\n",
      "  control → trimmed, longitudinal, use, characteristics, utilizing\n",
      "  → Expansion terms: boundary, characteristics, flow, laminar, layer, longitudinal, number, transfer, trimmed, use, utilizing, wall\n",
      "\n",
      "'compressible flow nozzle':\n",
      "  compressible → incompressible, fluid, laminar, gradient, viscosity\n",
      "  flow → layer, boundary, pressure, number, shock\n",
      "  nozzle → nozzles, exit, jet, stream, base\n",
      "  → Expansion terms: base, boundary, exit, fluid, gradient, incompressible, jet, laminar, layer, nozzles, number, pressure, shock, stream, viscosity\n",
      "\n",
      "'combustion chamber injection':\n",
      "  combustion → products, ignition, methane, flame, bluff\n",
      "  chamber → transversely, interacted, barrel, ms, 130\n",
      "  injection → helium, mass, porous, gas, wall\n",
      "  → Expansion terms: 130, barrel, bluff, flame, gas, helium, ignition, interacted, mass, methane, ms, porous, products, transversely, wall\n",
      "\n",
      "'laminar turbulent transition':\n",
      "  laminar → layer, boundary, transfer, heat, turbulent\n",
      "  turbulent → laminar, layers, layer, boundary, reynolds\n",
      "  transition → reynolds, roughness, cooling, number, boundary\n",
      "  → Expansion terms: boundary, cooling, heat, laminar, layer, layers, number, reynolds, roughness, transfer, turbulent\n",
      "\n",
      "'fatigue crack growth':\n",
      "  fatigue → life, structural, behaviour, random, damage\n",
      "  growth → wagner, gross, spheres, behind, exponential\n",
      "  → Expansion terms: behaviour, behind, damage, exponential, gross, life, random, spheres, structural, wagner\n",
      "\n",
      "'wing tip vortices':\n",
      "  wing → body, wings, lift, ratio, investigation\n",
      "  tip → source, radius, extreme, appropriate, fins\n",
      "  vortices → vortex, downwash, combinations, trailing, behind\n",
      "  → Expansion terms: appropriate, behind, body, combinations, downwash, extreme, fins, investigation, lift, radius, ratio, source, trailing, vortex, wings\n",
      "\n",
      "'propulsion efficiency':\n",
      "  propulsion → rockets, interplanetary, systems, reliability, status\n",
      "  efficiency → production, nacelle, equivalence, addition, source\n",
      "  → Expansion terms: addition, equivalence, interplanetary, nacelle, production, reliability, rockets, source, status, systems\n"
     ]
    }
   ],
   "source": [
    "#TODO: Identify neighborhood terms for queries_assignment3\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def get_query_neighborhood_terms(query: str, \n",
    "                                 C_normalized: np.ndarray, \n",
    "                                 vocab: Dict[str, int], \n",
    "                                 k: int = 5,\n",
    "                                 threshold: float = 0.0) -> Dict[str, List[Tuple[str, float]]]:\n",
    "   \n",
    "    query_terms = query.lower().split()\n",
    "    idx_to_term = {idx: term for term, idx in vocab.items()}\n",
    "    \n",
    "    neighborhood = {}\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in vocab:\n",
    "            idx = vocab[term]\n",
    "            correlations = C_normalized[idx, :]\n",
    "            \n",
    "            # Get top k terms excluding the term itself\n",
    "            top_indices = np.argsort(correlations)[::-1]\n",
    "            \n",
    "            neighbors = []\n",
    "            for i in top_indices:\n",
    "                if i != idx and correlations[i] >= threshold:\n",
    "                    neighbors.append((idx_to_term[i], correlations[i]))\n",
    "                    if len(neighbors) >= k:\n",
    "                        break\n",
    "            \n",
    "            neighborhood[term] = neighbors\n",
    "    \n",
    "    return neighborhood\n",
    "\n",
    "def get_expanded_query_terms(query: str,\n",
    "                             C_normalized: np.ndarray,\n",
    "                             vocab: Dict[str, int],\n",
    "                             k: int = 5,\n",
    "                             threshold: float = 0.0) -> Set[str]:\n",
    "    neighborhood = get_query_neighborhood_terms(query, C_normalized, vocab, k, threshold)\n",
    "    \n",
    "    expansion_terms = set()\n",
    "    for neighbors in neighborhood.values():\n",
    "        expansion_terms.update([term for term, score in neighbors])\n",
    "    \n",
    "    return expansion_terms\n",
    "\n",
    "def expand_queries(queries: List[str],\n",
    "                   C_normalized: np.ndarray,\n",
    "                   vocab: Dict[str, int],\n",
    "                   k: int = 5,\n",
    "                   threshold: float = 0.0) -> Dict[str, Dict]:\n",
    "    results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        neighborhood = get_query_neighborhood_terms(query, C_normalized, vocab, k, threshold)\n",
    "        expansion_terms = get_expanded_query_terms(query, C_normalized, vocab, k, threshold)\n",
    "        \n",
    "        results[query] = {\n",
    "            'original_terms': query.lower().split(),\n",
    "            'neighborhood': neighborhood,\n",
    "            'expansion_terms': sorted(expansion_terms)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_query_expansion(query: str, \n",
    "                         result: Dict,\n",
    "                         show_scores: bool = True):\n",
    "    \"\"\"Pretty print query expansion results.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for term, neighbors in result['neighborhood'].items():\n",
    "        print(f\"\\n  '{term}' → neighborhood terms:\")\n",
    "        for neighbor, score in neighbors:\n",
    "            if show_scores:\n",
    "                print(f\"    - {neighbor} ({score:.4f})\")\n",
    "            else:\n",
    "                print(f\"    - {neighbor}\")\n",
    "    \n",
    "    print(f\"\\n  All expansion terms: {', '.join(result['expansion_terms'])}\")\n",
    "    print()\n",
    "\n",
    "for query in queries_assignment3:\n",
    "    print(f\"\\n'{query}':\")\n",
    "    \n",
    "    # Get both at once\n",
    "    result = expand_queries([query], C_basic_normalized, vocab, k=5)[query]\n",
    "    \n",
    "    for term, neighbors in result['neighborhood'].items():\n",
    "        neighbor_terms = [n[0] for n in neighbors]\n",
    "        print(f\"  {term} → {', '.join(neighbor_terms)}\")\n",
    "    \n",
    "    print(f\"  → Expansion terms: {', '.join(result['expansion_terms'])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
