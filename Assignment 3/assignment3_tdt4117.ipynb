{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39093417",
   "metadata": {},
   "source": [
    "# Assignment 3: Local Association Matrix \n",
    "\n",
    "**Student names**: Matiss Podins <br>\n",
    "**Group number**: 39 <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please read and follow these rules. Submissions that do not fulfill them may be returned.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. Submit in **.ipynb** format only.\n",
    "3. The assignment must be typed. Handwritten answers are not accepted.\n",
    "\n",
    "**Due date**: 12.10.2025 23:59\n",
    "\n",
    "### What you will do \n",
    "- Build a **local association matrix** from Cranfield collection.\n",
    "- Compute the **normalized association matrix**.\n",
    "- Use the normalized matrix to **identify neighborhood terms** for expansion for given queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready â€” just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0b1f",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10841b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7263b",
   "metadata": {},
   "source": [
    "## 3.1  Local association matrix\n",
    "\n",
    "For the given Cranfield document collection in cran.all.1400 construct a local association matrix to identify association clusters. Use the docs variable with the parsed file. Omit stopwords in the STOPWORDS list given below from the vocabulary. \n",
    "\n",
    "\n",
    "The correlation factors $c_{u,v}$ between any pair of terms $w_u$ and $w_v$ are defined as  \n",
    "$c_{u,v} = \\sum_{d_j \\in D} f_{u,j} \\cdot f_{v,j}$  \n",
    "\n",
    "$f_{u,j}$ is the raw term frequency of $w_u$ in document $d_j$.\n",
    "\n",
    "### Weighting variants: **scalar** and **metric**\n",
    "\n",
    "Add two alternative weighting schemes for the matrix (only the formula for assigning the matrix cell value changes):\n",
    "\n",
    "- **Metric weighting** :\n",
    "Let $w_u(n,j)$ and $w_v(m,j)$ denote the $n$-th and $m$-th occurrences of terms $w_u$ and $w_v$ in document $d_j$.  \n",
    "Define a distance function $r(w_u(n,j), w_v(m,j))$ (e.g., $r(i,k) = 1 + |i - k|$).  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\sum_{d_j \\in D} \\sum_n \\sum_m \\frac{1}{r(w_u(n,j), w_v(m,j))}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Scalar weighting** :\n",
    "Let $\\vec{s}_u = \\langle c_{u,x_1}, c_{u,x_2}, \\dots, c_{u,x_n} \\rangle$ be the neighborhood vector of term $w_u$, and similarly for $w_v$.  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\frac{\\vec{s}_u \\cdot \\vec{s}_v}{|\\vec{s}_u| \\cdot |\\vec{s}_v|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9433bbe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 162\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Build all three association matrices\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m vocab, C_basic, C_metric, C_scalar \u001b[38;5;241m=\u001b[39m build_association_matrices(tokenized_documents)\n",
      "Cell \u001b[1;32mIn[32], line 137\u001b[0m, in \u001b[0;36mbuild_association_matrices\u001b[1;34m(tokenized_documents)\u001b[0m\n\u001b[0;32m    134\u001b[0m doc_term_positions \u001b[38;5;241m=\u001b[39m compute_term_positions(tokenized_documents, vocab)\n\u001b[0;32m    135\u001b[0m C_metric \u001b[38;5;241m=\u001b[39m metric_correlation_matrix(doc_term_positions, vocab_size)\n\u001b[1;32m--> 137\u001b[0m C_scalar \u001b[38;5;241m=\u001b[39m scalar_correlation_matrix(C_basic)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vocab, C_basic, C_metric, C_scalar\n",
      "Cell \u001b[1;32mIn[32], line 118\u001b[0m, in \u001b[0;36mscalar_correlation_matrix\u001b[1;34m(C_basic)\u001b[0m\n\u001b[0;32m    115\u001b[0m s_u \u001b[38;5;241m=\u001b[39m C_basic[u, :]\n\u001b[0;32m    116\u001b[0m s_v \u001b[38;5;241m=\u001b[39m C_basic[v, :]\n\u001b[1;32m--> 118\u001b[0m norm_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(s_u)\n\u001b[0;32m    119\u001b[0m norm_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(s_v)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_u \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m norm_v \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2552\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2550\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2552\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdot(x)\n\u001b[0;32m   2553\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Construct a local association matrix for the cranfield collection. Use both weighting variants.\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "# Your code here\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "\n",
    "def tokenization(docs):\n",
    "    result = []\n",
    "    \n",
    "    for doc in docs.values():\n",
    "        # Combine title and abstract\n",
    "        text = doc[\"title\"] + \" \" + doc[\"abstract\"]\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation and split into words\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        \n",
    "        # Split into tokens\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in STOPWORDS and token.strip()]\n",
    "        \n",
    "        result.append(filtered_tokens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_documents = tokenization(docs)\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_documents: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"Build a vocabulary mapping terms to indices.\"\"\"\n",
    "    vocab = set()\n",
    "    for tokens in tokenized_documents:\n",
    "        vocab.update(tokens)\n",
    "    return {term: idx for idx, term in enumerate(sorted(vocab))}\n",
    "\n",
    "def compute_term_frequencies(tokenized_documents: List[List[str]], \n",
    "                            vocab: Dict[str, int]) -> List[Dict[int, int]]:\n",
    "    \"\"\"Compute raw term frequencies for each document.\"\"\"\n",
    "    doc_term_freqs = []\n",
    "    for tokens in tokenized_documents:\n",
    "        freq = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                freq[vocab[token]] += 1\n",
    "        doc_term_freqs.append(dict(freq))\n",
    "    return doc_term_freqs\n",
    "\n",
    "def compute_term_positions(tokenized_documents: List[List[str]], \n",
    "                          vocab: Dict[str, int]) -> List[Dict[int, List[int]]]:\n",
    "    \"\"\"Compute positions of each term in each document.\"\"\"\n",
    "    doc_term_positions = []\n",
    "    for tokens in tokenized_documents:\n",
    "        positions = defaultdict(list)\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token in vocab:\n",
    "                positions[vocab[token]].append(pos)\n",
    "        doc_term_positions.append(dict(positions))\n",
    "    return doc_term_positions\n",
    "\n",
    "def basic_correlation_matrix(doc_term_freqs: List[Dict[int, int]], \n",
    "                             vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"Compute basic correlation matrix using raw term frequencies.\"\"\"\n",
    "    C = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for freq_dict in doc_term_freqs:\n",
    "        terms = list(freq_dict.keys())\n",
    "        for u in terms:\n",
    "            for v in terms:\n",
    "                C[u, v] += freq_dict[u] * freq_dict[v]\n",
    "    \n",
    "    return C\n",
    "\n",
    "def metric_correlation_matrix(doc_term_positions: List[Dict[int, List[int]]], \n",
    "                              vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"Compute metric-weighted correlation matrix (optimized).\"\"\"\n",
    "    C = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for pos_dict in doc_term_positions:\n",
    "        terms = list(pos_dict.keys())\n",
    "        for u in terms:\n",
    "            pos_u = np.array(pos_dict[u])\n",
    "            for v in terms:\n",
    "                pos_v = np.array(pos_dict[v])\n",
    "                # Vectorized distance calculation\n",
    "                distances = 1 + np.abs(pos_u[:, None] - pos_v[None, :])\n",
    "                C[u, v] += np.sum(1.0 / distances)\n",
    "    \n",
    "    return C\n",
    "\n",
    "def scalar_correlation_matrix(C_basic: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute scalar-weighted correlation matrix using cosine similarity.\"\"\"\n",
    "    vocab_size = C_basic.shape[0]\n",
    "    C_scalar = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for u in range(vocab_size):\n",
    "        for v in range(vocab_size):\n",
    "            s_u = C_basic[u, :]\n",
    "            s_v = C_basic[v, :]\n",
    "            \n",
    "            norm_u = np.linalg.norm(s_u)\n",
    "            norm_v = np.linalg.norm(s_v)\n",
    "            \n",
    "            if norm_u > 0 and norm_v > 0:\n",
    "                C_scalar[u, v] = np.dot(s_u, s_v) / (norm_u * norm_v)\n",
    "    \n",
    "    return C_scalar\n",
    "\n",
    "def build_association_matrices(tokenized_documents: List[List[str]]) -> Tuple:\n",
    "    \"\"\"Build all three types of association matrices.\"\"\"\n",
    "    vocab = build_vocabulary(tokenized_documents)\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    doc_term_freqs = compute_term_frequencies(tokenized_documents, vocab)\n",
    "    C_basic = basic_correlation_matrix(doc_term_freqs, vocab_size)\n",
    "    \n",
    "    doc_term_positions = compute_term_positions(tokenized_documents, vocab)\n",
    "    C_metric = metric_correlation_matrix(doc_term_positions, vocab_size)\n",
    "    \n",
    "    C_scalar = scalar_correlation_matrix(C_basic)\n",
    "    \n",
    "    return vocab, C_basic, C_metric, C_scalar\n",
    "\n",
    "\n",
    "def find_top_associations(C: np.ndarray, vocab: Dict[str, int], \n",
    "                         term: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Find top k associated terms for a given term.\"\"\"\n",
    "    if term not in vocab:\n",
    "        return []\n",
    "    \n",
    "    idx = vocab[term]\n",
    "    correlations = C[idx, :]\n",
    "    \n",
    "    idx_to_term = {idx: term for term, idx in vocab.items()}\n",
    "    top_indices = np.argsort(correlations)[::-1]\n",
    "    \n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        if i != idx and len(results) < k:\n",
    "            results.append((idx_to_term[i], correlations[i]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build all three association matrices\n",
    "vocab, C_basic, C_metric, C_scalar = build_association_matrices(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55f9bc",
   "metadata": {},
   "source": [
    "## 3.2 Normalized association matrix\n",
    "\n",
    "Compute the normalized association matrix from the unnormalized matrix computed above. \n",
    "\n",
    "To normalize the matrix use the following formula: <br>\n",
    "$c'_{u,v} = \\frac{c_{u,v}}{c_{u,u} + c_{v,v} - c_{u,v}}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f5964",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C_basic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m C_normalized\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Normalize the association matrices\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m C_basic_normalized \u001b[38;5;241m=\u001b[39m normalize_association_matrix(C_basic)\n\u001b[0;32m     21\u001b[0m C_metric_normalized \u001b[38;5;241m=\u001b[39m normalize_association_matrix(C_metric)\n\u001b[0;32m     22\u001b[0m C_scalar_normalized \u001b[38;5;241m=\u001b[39m normalize_association_matrix(C_scalar)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C_basic' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: Compute the normalized association matrix \n",
    "\n",
    "# Your code here\n",
    "def normalize_association_matrix(C: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize association matrix using Dice coefficient.\n",
    "    c'_{u,v} = c_{u,v} / (c_{u,u} + c_{v,v} - c_{u,v})\n",
    "    \"\"\"\n",
    "    vocab_size = C.shape[0]\n",
    "    C_normalized = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for u in range(vocab_size):\n",
    "        for v in range(vocab_size):\n",
    "            denominator = C[u, u] + C[v, v] - C[u, v]\n",
    "            if denominator > 0:\n",
    "                C_normalized[u, v] = C[u, v] / denominator\n",
    "    \n",
    "    return C_normalized\n",
    "\n",
    "# Normalize the association matrices\n",
    "C_basic_normalized = normalize_association_matrix(C_basic)\n",
    "C_metric_normalized = normalize_association_matrix(C_metric)\n",
    "C_scalar_normalized = normalize_association_matrix(C_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6e8c0",
   "metadata": {},
   "source": [
    "## 3.3 Neighborhood terms\n",
    "\n",
    "With the help of the normalized local association matrix, identify the neighborhood terms that should be used for expansion for the following queries (queries_assignment3):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "queries_assignment3 = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a58c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'gas pressure':\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'C_basic_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries_assignment3:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     neighborhood \u001b[38;5;241m=\u001b[39m get_query_neighborhood_terms(query, C_basic_normalized, vocab, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m term, neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhood\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     91\u001b[0m         neighbor_terms \u001b[38;5;241m=\u001b[39m [n[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m neighbors]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C_basic_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: Identify neighborhood terms for queries_assignment3\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def get_query_neighborhood_terms(query: str, \n",
    "                                 C_normalized: np.ndarray, \n",
    "                                 vocab: Dict[str, int], \n",
    "                                 k: int = 5,\n",
    "                                 threshold: float = 0.0) -> Dict[str, List[Tuple[str, float]]]:\n",
    "   \n",
    "    query_terms = query.lower().split()\n",
    "    idx_to_term = {idx: term for term, idx in vocab.items()}\n",
    "    \n",
    "    neighborhood = {}\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in vocab:\n",
    "            idx = vocab[term]\n",
    "            correlations = C_normalized[idx, :]\n",
    "            \n",
    "            # Get top k terms excluding the term itself\n",
    "            top_indices = np.argsort(correlations)[::-1]\n",
    "            \n",
    "            neighbors = []\n",
    "            for i in top_indices:\n",
    "                if i != idx and correlations[i] >= threshold:\n",
    "                    neighbors.append((idx_to_term[i], correlations[i]))\n",
    "                    if len(neighbors) >= k:\n",
    "                        break\n",
    "            \n",
    "            neighborhood[term] = neighbors\n",
    "    \n",
    "    return neighborhood\n",
    "\n",
    "def get_expanded_query_terms(query: str,\n",
    "                             C_normalized: np.ndarray,\n",
    "                             vocab: Dict[str, int],\n",
    "                             k: int = 5,\n",
    "                             threshold: float = 0.0) -> Set[str]:\n",
    "    neighborhood = get_query_neighborhood_terms(query, C_normalized, vocab, k, threshold)\n",
    "    \n",
    "    expansion_terms = set()\n",
    "    for neighbors in neighborhood.values():\n",
    "        expansion_terms.update([term for term, score in neighbors])\n",
    "    \n",
    "    return expansion_terms\n",
    "\n",
    "def expand_queries(queries: List[str],\n",
    "                   C_normalized: np.ndarray,\n",
    "                   vocab: Dict[str, int],\n",
    "                   k: int = 5,\n",
    "                   threshold: float = 0.0) -> Dict[str, Dict]:\n",
    "    results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        neighborhood = get_query_neighborhood_terms(query, C_normalized, vocab, k, threshold)\n",
    "        expansion_terms = get_expanded_query_terms(query, C_normalized, vocab, k, threshold)\n",
    "        \n",
    "        results[query] = {\n",
    "            'original_terms': query.lower().split(),\n",
    "            'neighborhood': neighborhood,\n",
    "            'expansion_terms': sorted(expansion_terms)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_query_expansion(query: str, \n",
    "                         result: Dict,\n",
    "                         show_scores: bool = True):\n",
    "    \"\"\"Pretty print query expansion results.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for term, neighbors in result['neighborhood'].items():\n",
    "        print(f\"\\n  '{term}' â†’ neighborhood terms:\")\n",
    "        for neighbor, score in neighbors:\n",
    "            if show_scores:\n",
    "                print(f\"    - {neighbor} ({score:.4f})\")\n",
    "            else:\n",
    "                print(f\"    - {neighbor}\")\n",
    "    \n",
    "    print(f\"\\n  All expansion terms: {', '.join(result['expansion_terms'])}\")\n",
    "    print()\n",
    "\n",
    "for query in queries_assignment3:\n",
    "    print(f\"\\n'{query}':\")\n",
    "    \n",
    "    neighborhood = get_query_neighborhood_terms(query, C_basic_normalized, vocab, k=5)\n",
    "    \n",
    "    for term, neighbors in neighborhood.items():\n",
    "        neighbor_terms = [n[0] for n in neighbors]\n",
    "        print(f\"  {term} â†’ {', '.join(neighbor_terms)}\")\n",
    "    \n",
    "    expansion_terms = get_expanded_query_terms(query, C_basic_normalized, vocab, k=5)\n",
    "    print(f\"  â†’ Expansion terms: {', '.join(sorted(expansion_terms))}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
